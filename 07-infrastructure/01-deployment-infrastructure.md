# Ø®Ø·Ø© Ø§Ù„Ù†Ø´Ø± ÙˆØ§Ù„Ø¨Ù†ÙŠØ© Ø§Ù„ØªØ­ØªÙŠØ© - Ù…Ù†ØµØ© Ù†Ø§Ø¦Ø¨Ùƒ

## ğŸ—ï¸ **Ù†Ø¸Ø±Ø© Ø¹Ø§Ù…Ø©**

Ù‡Ø°Ø§ Ø§Ù„Ù…Ù„Ù ÙŠØ­Ø¯Ø¯ Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© Ø´Ø§Ù…Ù„Ø© Ù„Ù†Ø´Ø± Ù…Ù†ØµØ© Ù†Ø§Ø¦Ø¨Ùƒ Ø¹Ù„Ù‰ Google Cloud Platform Ù…Ø¹ Ø¶Ù…Ø§Ù† Ø§Ù„Ø§Ø³ØªÙ‚Ø±Ø§Ø±ØŒ Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ø¹Ø§Ù„ÙŠØŒ ÙˆØ§Ù„ØªÙˆØ³Ø¹ Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ.

---

## ğŸ¯ **Ø£Ù‡Ø¯Ø§Ù Ø§Ù„Ø¨Ù†ÙŠØ© Ø§Ù„ØªØ­ØªÙŠØ©**

### **Ø§Ù„Ø£Ù‡Ø¯Ø§Ù Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©:**
1. **Ø§Ù„ØªÙˆÙØ± Ø§Ù„Ø¹Ø§Ù„ÙŠ** - 99.9% uptime
2. **Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù…ØªÙ…ÙŠØ²** - Ø§Ø³ØªØ¬Ø§Ø¨Ø© Ø£Ù‚Ù„ Ù…Ù† 200ms
3. **Ø§Ù„ØªÙˆØ³Ø¹ Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ** - Ø¯Ø¹Ù… Ù†Ù…Ùˆ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ†
4. **Ø§Ù„Ø£Ù…Ø§Ù† Ø§Ù„Ù…ØªÙ‚Ø¯Ù…** - Ø­Ù…Ø§ÙŠØ© Ø´Ø§Ù…Ù„Ø© Ù„Ù„Ø¨ÙŠØ§Ù†Ø§Øª
5. **Ø§Ù„ØªÙƒÙ„ÙØ© Ø§Ù„Ù…Ø­Ø³Ù†Ø©** - Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙØ¹Ø§Ù„ Ù„Ù„Ù…ÙˆØ§Ø±Ø¯

### **Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª Ø§Ù„ØªÙ‚Ù†ÙŠØ©:**
- **Ø¯Ø¹Ù… 10,000 Ù…Ø³ØªØ®Ø¯Ù… Ù…ØªØ²Ø§Ù…Ù†**
- **Ù…Ø¹Ø§Ù„Ø¬Ø© 1,000 Ø·Ù„Ø¨/Ø«Ø§Ù†ÙŠØ©**
- **ØªØ®Ø²ÙŠÙ† 1TB Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª**
- **Ù†Ø³Ø® Ø§Ø­ØªÙŠØ§Ø·ÙŠ ÙŠÙˆÙ…ÙŠ**
- **Ø§Ø³ØªØ¹Ø§Ø¯Ø© Ø®Ù„Ø§Ù„ 15 Ø¯Ù‚ÙŠÙ‚Ø©**

---

## â˜ï¸ **Ù…Ø¹Ù…Ø§Ø±ÙŠØ© Google Cloud**

### **1. Ù†Ø¸Ø±Ø© Ø¹Ø§Ù…Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø¨Ù†ÙŠØ©**

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   Cloud CDN     â”‚
                    â”‚   (Global)      â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚ Load Balancer   â”‚
                    â”‚ (Global HTTPS)  â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚             â”‚             â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”
        â”‚   Frontend   â”‚ â”‚   API   â”‚ â”‚   Admin   â”‚
        â”‚ (Cloud Run)  â”‚ â”‚Gateway  â”‚ â”‚  Panel    â”‚
        â”‚              â”‚ â”‚         â”‚ â”‚           â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚ Microservices   â”‚
                    â”‚   (Cloud Run)   â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚                 â”‚                 â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”
    â”‚ Cloud SQL    â”‚ â”‚ Cloud Storage   â”‚ â”‚  Redis    â”‚
    â”‚ (PostgreSQL) â”‚ â”‚   (Files)       â”‚ â”‚ (Memstore)â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### **2. Ù…ÙƒÙˆÙ†Ø§Øª Ø§Ù„Ø¨Ù†ÙŠØ© Ø§Ù„ØªØ­ØªÙŠØ©**

#### **Ø§Ù„Ø­ÙˆØ³Ø¨Ø© (Compute):**
```yaml
# Cloud Run Services
services:
  - name: naebak-frontend
    image: gcr.io/naebak-472518/frontend:latest
    cpu: 2
    memory: 4Gi
    min_instances: 2
    max_instances: 100
    concurrency: 1000
    
  - name: naebak-gateway
    image: gcr.io/naebak-472518/gateway:latest
    cpu: 1
    memory: 2Gi
    min_instances: 2
    max_instances: 50
    concurrency: 1000
    
  - name: naebak-auth-service
    image: gcr.io/naebak-472518/auth-service:latest
    cpu: 1
    memory: 2Gi
    min_instances: 1
    max_instances: 20
    concurrency: 500
    
  - name: naebak-complaints-service
    image: gcr.io/naebak-472518/complaints-service:latest
    cpu: 1
    memory: 2Gi
    min_instances: 1
    max_instances: 30
    concurrency: 500
```

#### **Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª:**
```yaml
# Cloud SQL Instances
databases:
  - name: naebak-main-db
    type: PostgreSQL
    version: 14
    tier: db-custom-4-16384  # 4 vCPUs, 16GB RAM
    storage: 100GB
    backup_enabled: true
    backup_time: "03:00"
    high_availability: true
    region: us-central1
    
  - name: naebak-analytics-db
    type: PostgreSQL
    version: 14
    tier: db-custom-2-8192   # 2 vCPUs, 8GB RAM
    storage: 50GB
    backup_enabled: true
    read_replicas: 2

# Redis (Memorystore)
cache:
  - name: naebak-redis
    type: Redis
    version: 6.x
    tier: STANDARD_HA
    memory: 5GB
    region: us-central1
    auth_enabled: true
```

#### **Ø§Ù„ØªØ®Ø²ÙŠÙ†:**
```yaml
# Cloud Storage Buckets
storage:
  - name: naebak-media-files
    location: us-central1
    storage_class: STANDARD
    versioning: true
    lifecycle:
      - action: DELETE
        condition:
          age: 365
          
  - name: naebak-backups
    location: us-central1
    storage_class: NEARLINE
    versioning: true
    lifecycle:
      - action: DELETE
        condition:
          age: 2555  # 7 years
          
  - name: naebak-logs
    location: us-central1
    storage_class: COLDLINE
    lifecycle:
      - action: DELETE
        condition:
          age: 90
```

---

## ğŸš€ **Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© Ø§Ù„Ù†Ø´Ø±**

### **1. CI/CD Pipeline**

#### **GitHub Actions Workflow:**
```yaml
# .github/workflows/deploy.yml
name: Deploy to Google Cloud Run

on:
  push:
    branches: [main, staging]
  pull_request:
    branches: [main]

env:
  PROJECT_ID: naebak-472518
  GAR_LOCATION: us-central1
  REPOSITORY: naebak-repo
  SERVICE: naebak-frontend
  REGION: us-central1

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v3
        
      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
          
      - name: Install dependencies
        run: |
          pip install -r requirements.txt
          pip install pytest pytest-cov
          
      - name: Run tests
        run: |
          pytest --cov=./ --cov-report=xml
          
      - name: Security scan
        run: |
          pip install bandit safety
          bandit -r . -f json -o bandit-report.json
          safety check
          
  build-and-deploy:
    needs: test
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    
    steps:
      - name: Checkout
        uses: actions/checkout@v3
        
      - name: Google Auth
        id: auth
        uses: google-github-actions/auth@v1
        with:
          credentials_json: '${{ secrets.GCP_SA_KEY }}'
          
      - name: Configure Docker
        run: gcloud auth configure-docker $GAR_LOCATION-docker.pkg.dev
        
      - name: Build Docker image
        run: |
          docker build -t $GAR_LOCATION-docker.pkg.dev/$PROJECT_ID/$REPOSITORY/$SERVICE:$GITHUB_SHA .
          docker build -t $GAR_LOCATION-docker.pkg.dev/$PROJECT_ID/$REPOSITORY/$SERVICE:latest .
          
      - name: Push Docker image
        run: |
          docker push $GAR_LOCATION-docker.pkg.dev/$PROJECT_ID/$REPOSITORY/$SERVICE:$GITHUB_SHA
          docker push $GAR_LOCATION-docker.pkg.dev/$PROJECT_ID/$REPOSITORY/$SERVICE:latest
          
      - name: Deploy to Cloud Run
        run: |
          gcloud run deploy $SERVICE \
            --image $GAR_LOCATION-docker.pkg.dev/$PROJECT_ID/$REPOSITORY/$SERVICE:$GITHUB_SHA \
            --platform managed \
            --region $REGION \
            --allow-unauthenticated \
            --set-env-vars="DATABASE_URL=${{ secrets.DATABASE_URL }}" \
            --set-env-vars="REDIS_URL=${{ secrets.REDIS_URL }}" \
            --memory=4Gi \
            --cpu=2 \
            --min-instances=2 \
            --max-instances=100 \
            --concurrency=1000 \
            --timeout=300
            
      - name: Update Traffic
        run: |
          gcloud run services update-traffic $SERVICE \
            --region $REGION \
            --to-latest
```

### **2. Blue-Green Deployment**

#### **Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© Ø§Ù„Ù†Ø´Ø± Ø§Ù„ØªØ¯Ø±ÙŠØ¬ÙŠ:**
```python
# deploy_manager.py
class BlueGreenDeployment:
    """Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ù†Ø´Ø± Ø§Ù„Ø£Ø²Ø±Ù‚-Ø§Ù„Ø£Ø®Ø¶Ø±"""
    
    def __init__(self, project_id, region):
        self.project_id = project_id
        self.region = region
        self.client = run_v1.ServicesClient()
    
    def deploy_green_version(self, service_name, image_url):
        """Ù†Ø´Ø± Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ø®Ø¶Ø±Ø§Ø¡ Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©"""
        
        # Ø¥Ù†Ø´Ø§Ø¡ revision Ø¬Ø¯ÙŠØ¯
        green_revision = f"{service_name}-green-{int(time.time())}"
        
        deployment_config = {
            'metadata': {
                'name': green_revision,
                'labels': {
                    'deployment-type': 'green',
                    'version': os.environ.get('GITHUB_SHA', 'latest')
                }
            },
            'spec': {
                'template': {
                    'spec': {
                        'containers': [{
                            'image': image_url,
                            'env': self.get_environment_variables(),
                            'resources': {
                                'limits': {
                                    'cpu': '2000m',
                                    'memory': '4Gi'
                                }
                            }
                        }]
                    }
                }
            }
        }
        
        # Ù†Ø´Ø± Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø© Ø¨Ø¯ÙˆÙ† traffic
        response = self.client.replace_service(
            parent=f"projects/{self.project_id}/locations/{self.region}",
            service=deployment_config
        )
        
        return green_revision
    
    def health_check_green(self, service_name, green_revision):
        """ÙØ­Øµ ØµØ­Ø© Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ø®Ø¶Ø±Ø§Ø¡"""
        health_checks = [
            self.check_service_health(service_name, green_revision),
            self.check_database_connectivity(green_revision),
            self.check_external_apis(green_revision),
            self.run_smoke_tests(green_revision)
        ]
        
        return all(health_checks)
    
    def switch_traffic_gradually(self, service_name, green_revision):
        """ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØªØ±Ø§ÙÙŠÙƒ ØªØ¯Ø±ÙŠØ¬ÙŠØ§Ù‹"""
        traffic_percentages = [10, 25, 50, 75, 100]
        
        for percentage in traffic_percentages:
            # ØªØ­ÙˆÙŠÙ„ Ù†Ø³Ø¨Ø© Ù…Ù† Ø§Ù„ØªØ±Ø§ÙÙŠÙƒ
            self.update_traffic_allocation(
                service_name, 
                green_revision, 
                percentage
            )
            
            # Ø§Ù†ØªØ¸Ø§Ø± ÙˆÙ…Ø±Ø§Ù‚Ø¨Ø©
            time.sleep(300)  # 5 Ø¯Ù‚Ø§Ø¦Ù‚
            
            # ÙØ­Øµ Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³
            if not self.monitor_metrics(service_name, green_revision):
                # rollback ÙÙŠ Ø­Ø§Ù„Ø© ÙˆØ¬ÙˆØ¯ Ù…Ø´Ø§ÙƒÙ„
                self.rollback_deployment(service_name)
                return False
        
        return True
    
    def cleanup_blue_version(self, service_name):
        """ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ø²Ø±Ù‚Ø§Ø¡ Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø©"""
        # Ø§Ù„Ø§Ø­ØªÙØ§Ø¸ Ø¨Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø© Ù„Ù…Ø¯Ø© 24 Ø³Ø§Ø¹Ø©
        # Ø«Ù… Ø­Ø°ÙÙ‡Ø§ ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹
        pass
```

### **3. Rollback Strategy**

#### **Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© Ø§Ù„ØªØ±Ø§Ø¬Ø¹:**
```python
class RollbackManager:
    """Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„ØªØ±Ø§Ø¬Ø¹ Ø¹Ù† Ø§Ù„Ù†Ø´Ø±"""
    
    def __init__(self, project_id, region):
        self.project_id = project_id
        self.region = region
    
    def auto_rollback_conditions(self):
        """Ø´Ø±ÙˆØ· Ø§Ù„ØªØ±Ø§Ø¬Ø¹ Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ"""
        return {
            'error_rate_threshold': 5,      # 5% error rate
            'response_time_threshold': 2000, # 2 seconds
            'availability_threshold': 99.5,  # 99.5% availability
            'memory_usage_threshold': 90,    # 90% memory usage
            'cpu_usage_threshold': 85        # 85% CPU usage
        }
    
    def monitor_deployment(self, service_name, revision):
        """Ù…Ø±Ø§Ù‚Ø¨Ø© Ø§Ù„Ù†Ø´Ø± Ù„Ù„ØªØ±Ø§Ø¬Ø¹ Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ"""
        conditions = self.auto_rollback_conditions()
        
        while True:
            metrics = self.get_service_metrics(service_name, revision)
            
            # ÙØ­Øµ Ù…Ø¹Ø¯Ù„ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡
            if metrics['error_rate'] > conditions['error_rate_threshold']:
                self.trigger_rollback(service_name, 'High error rate')
                break
            
            # ÙØ­Øµ ÙˆÙ‚Øª Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø©
            if metrics['avg_response_time'] > conditions['response_time_threshold']:
                self.trigger_rollback(service_name, 'High response time')
                break
            
            # ÙØ­Øµ Ø§Ù„ØªÙˆÙØ±
            if metrics['availability'] < conditions['availability_threshold']:
                self.trigger_rollback(service_name, 'Low availability')
                break
            
            time.sleep(60)  # ÙØ­Øµ ÙƒÙ„ Ø¯Ù‚ÙŠÙ‚Ø©
    
    def trigger_rollback(self, service_name, reason):
        """ØªÙ†ÙÙŠØ° Ø§Ù„ØªØ±Ø§Ø¬Ø¹"""
        logging.critical(f"Triggering rollback for {service_name}: {reason}")
        
        # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¢Ø®Ø± revision Ù…Ø³ØªÙ‚Ø±
        stable_revision = self.get_last_stable_revision(service_name)
        
        # ØªØ­ÙˆÙŠÙ„ ÙƒØ§Ù…Ù„ Ø§Ù„ØªØ±Ø§ÙÙŠÙƒ Ù„Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ù…Ø³ØªÙ‚Ø±Ø©
        self.client.update_service_traffic(
            service_name=service_name,
            traffic_allocation={stable_revision: 100}
        )
        
        # Ø¥Ø±Ø³Ø§Ù„ ØªÙ†Ø¨ÙŠÙ‡Ø§Øª
        self.send_rollback_alerts(service_name, reason)
        
        # ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø­Ø¯Ø«
        self.log_rollback_event(service_name, reason, stable_revision)
```

---

## ğŸ“Š **Ù…Ø±Ø§Ù‚Ø¨Ø© Ø§Ù„Ø£Ø¯Ø§Ø¡ ÙˆØ§Ù„ØªÙ†Ø¨ÙŠÙ‡Ø§Øª**

### **1. Google Cloud Monitoring**

#### **Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù…Ø±Ø§Ù‚Ø¨Ø©:**
```yaml
# monitoring.yaml
monitoring:
  dashboards:
    - name: "Naebak Overview"
      widgets:
        - title: "Request Rate"
          type: line_chart
          metrics:
            - "run.googleapis.com/request_count"
        - title: "Response Time"
          type: line_chart
          metrics:
            - "run.googleapis.com/request_latencies"
        - title: "Error Rate"
          type: line_chart
          metrics:
            - "run.googleapis.com/request_count"
          filter: 'response_code!="200"'
        - title: "CPU Utilization"
          type: line_chart
          metrics:
            - "run.googleapis.com/container/cpu/utilizations"
        - title: "Memory Usage"
          type: line_chart
          metrics:
            - "run.googleapis.com/container/memory/utilizations"

  alerts:
    - name: "High Error Rate"
      condition:
        metric: "run.googleapis.com/request_count"
        filter: 'response_code!="200"'
        threshold: 5  # 5% error rate
        duration: 300  # 5 minutes
      notification_channels:
        - "projects/naebak-472518/notificationChannels/email-alerts"
        - "projects/naebak-472518/notificationChannels/slack-alerts"
    
    - name: "High Response Time"
      condition:
        metric: "run.googleapis.com/request_latencies"
        threshold: 2000  # 2 seconds
        duration: 300
      notification_channels:
        - "projects/naebak-472518/notificationChannels/email-alerts"
    
    - name: "Service Down"
      condition:
        metric: "run.googleapis.com/request_count"
        threshold: 0
        duration: 180  # 3 minutes
      notification_channels:
        - "projects/naebak-472518/notificationChannels/emergency-alerts"
```

### **2. Custom Metrics**

#### **Ù…Ù‚Ø§ÙŠÙŠØ³ Ù…Ø®ØµØµØ© Ù„Ù„ØªØ·Ø¨ÙŠÙ‚:**
```python
from google.cloud import monitoring_v3

class CustomMetrics:
    """Ù…Ù‚Ø§ÙŠÙŠØ³ Ù…Ø®ØµØµØ© Ù„Ù„ØªØ·Ø¨ÙŠÙ‚"""
    
    def __init__(self, project_id):
        self.project_id = project_id
        self.client = monitoring_v3.MetricServiceClient()
        self.project_name = f"projects/{project_id}"
    
    def create_custom_metrics(self):
        """Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù‚Ø§ÙŠÙŠØ³ Ù…Ø®ØµØµØ©"""
        metrics = [
            {
                'type': 'custom.googleapis.com/naebak/active_users',
                'display_name': 'Active Users',
                'description': 'Number of active users in the last 5 minutes',
                'metric_kind': monitoring_v3.MetricDescriptor.MetricKind.GAUGE,
                'value_type': monitoring_v3.MetricDescriptor.ValueType.INT64
            },
            {
                'type': 'custom.googleapis.com/naebak/complaints_created',
                'display_name': 'Complaints Created',
                'description': 'Number of complaints created per minute',
                'metric_kind': monitoring_v3.MetricDescriptor.MetricKind.GAUGE,
                'value_type': monitoring_v3.MetricDescriptor.ValueType.INT64
            },
            {
                'type': 'custom.googleapis.com/naebak/database_connections',
                'display_name': 'Database Connections',
                'description': 'Number of active database connections',
                'metric_kind': monitoring_v3.MetricDescriptor.MetricKind.GAUGE,
                'value_type': monitoring_v3.MetricDescriptor.ValueType.INT64
            }
        ]
        
        for metric_config in metrics:
            descriptor = monitoring_v3.MetricDescriptor(
                type=metric_config['type'],
                display_name=metric_config['display_name'],
                description=metric_config['description'],
                metric_kind=metric_config['metric_kind'],
                value_type=metric_config['value_type']
            )
            
            self.client.create_metric_descriptor(
                name=self.project_name,
                metric_descriptor=descriptor
            )
    
    def send_metric(self, metric_type, value, labels=None):
        """Ø¥Ø±Ø³Ø§Ù„ Ù‚ÙŠÙ…Ø© Ù…Ù‚ÙŠØ§Ø³"""
        series = monitoring_v3.TimeSeries()
        series.metric.type = f"custom.googleapis.com/naebak/{metric_type}"
        
        if labels:
            for key, value in labels.items():
                series.metric.labels[key] = value
        
        series.resource.type = "global"
        
        now = time.time()
        seconds = int(now)
        nanos = int((now - seconds) * 10 ** 9)
        interval = monitoring_v3.TimeInterval(
            {"end_time": {"seconds": seconds, "nanos": nanos}}
        )
        point = monitoring_v3.Point(
            {"interval": interval, "value": {"int64_value": value}}
        )
        series.points = [point]
        
        self.client.create_time_series(
            name=self.project_name,
            time_series=[series]
        )
```

### **3. Health Checks**

#### **ÙØ­ÙˆØµØ§Øª Ø§Ù„ØµØ­Ø© Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©:**
```python
class AdvancedHealthCheck:
    """ÙØ­ÙˆØµØ§Øª ØµØ­Ø© Ù…ØªÙ‚Ø¯Ù…Ø©"""
    
    def __init__(self):
        self.checks = [
            self.check_database_health,
            self.check_redis_health,
            self.check_external_apis,
            self.check_file_storage,
            self.check_memory_usage,
            self.check_disk_space
        ]
    
    def check_database_health(self):
        """ÙØ­Øµ ØµØ­Ø© Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
        try:
            start_time = time.time()
            
            # ÙØ­Øµ Ø§Ù„Ø§ØªØµØ§Ù„
            with connection.cursor() as cursor:
                cursor.execute("SELECT 1")
                result = cursor.fetchone()
            
            response_time = (time.time() - start_time) * 1000
            
            # ÙØ­Øµ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ø§Ù„Ø¨Ø·ÙŠØ¦Ø©
            slow_queries = self.get_slow_queries_count()
            
            # ÙØ­Øµ Ø§Ù„Ø§ØªØµØ§Ù„Ø§Øª Ø§Ù„Ù†Ø´Ø·Ø©
            active_connections = self.get_active_connections_count()
            
            return {
                'status': 'healthy' if result and response_time < 100 else 'unhealthy',
                'response_time_ms': response_time,
                'slow_queries': slow_queries,
                'active_connections': active_connections,
                'max_connections': settings.DATABASES['default']['OPTIONS'].get('MAX_CONNS', 20)
            }
        except Exception as e:
            return {
                'status': 'unhealthy',
                'error': str(e)
            }
    
    def check_redis_health(self):
        """ÙØ­Øµ ØµØ­Ø© Redis"""
        try:
            start_time = time.time()
            
            # ÙØ­Øµ Ø§Ù„Ø§ØªØµØ§Ù„
            cache.set('health_check', 'ok', timeout=10)
            result = cache.get('health_check')
            
            response_time = (time.time() - start_time) * 1000
            
            # ÙØ­Øµ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø°Ø§ÙƒØ±Ø©
            redis_client = cache._cache.get_client()
            memory_info = redis_client.info('memory')
            
            return {
                'status': 'healthy' if result == 'ok' else 'unhealthy',
                'response_time_ms': response_time,
                'memory_used_mb': memory_info['used_memory'] / 1024 / 1024,
                'memory_peak_mb': memory_info['used_memory_peak'] / 1024 / 1024
            }
        except Exception as e:
            return {
                'status': 'unhealthy',
                'error': str(e)
            }
    
    def check_external_apis(self):
        """ÙØ­Øµ Ø§Ù„Ø®Ø¯Ù…Ø§Øª Ø§Ù„Ø®Ø§Ø±Ø¬ÙŠØ©"""
        external_services = {
            'sms_service': 'https://api.sms-service.com/health',
            'email_service': 'https://api.email-service.com/health',
            'maps_api': 'https://maps.googleapis.com/maps/api/geocode/json?address=test'
        }
        
        results = {}
        for service_name, url in external_services.items():
            try:
                start_time = time.time()
                response = requests.get(url, timeout=5)
                response_time = (time.time() - start_time) * 1000
                
                results[service_name] = {
                    'status': 'healthy' if response.status_code == 200 else 'unhealthy',
                    'response_time_ms': response_time,
                    'status_code': response.status_code
                }
            except Exception as e:
                results[service_name] = {
                    'status': 'unhealthy',
                    'error': str(e)
                }
        
        return results
    
    def generate_health_report(self):
        """Ø¥Ù†Ø´Ø§Ø¡ ØªÙ‚Ø±ÙŠØ± ØµØ­Ø© Ø´Ø§Ù…Ù„"""
        report = {
            'timestamp': timezone.now().isoformat(),
            'overall_status': 'healthy',
            'checks': {}
        }
        
        for check in self.checks:
            check_name = check.__name__.replace('check_', '')
            try:
                result = check()
                report['checks'][check_name] = result
                
                # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø­Ø§Ù„Ø© Ø§Ù„Ø¹Ø§Ù…Ø©
                if isinstance(result, dict) and result.get('status') == 'unhealthy':
                    report['overall_status'] = 'unhealthy'
                elif isinstance(result, dict) and any(
                    service.get('status') == 'unhealthy' 
                    for service in result.values() 
                    if isinstance(service, dict)
                ):
                    report['overall_status'] = 'degraded'
                    
            except Exception as e:
                report['checks'][check_name] = {
                    'status': 'error',
                    'error': str(e)
                }
                report['overall_status'] = 'unhealthy'
        
        return report
```

---

## ğŸ”„ **Ø§Ù„ØªÙˆØ³Ø¹ Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ (Auto Scaling)**

### **1. Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Cloud Run Auto Scaling**

#### **Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© Ø§Ù„ØªÙˆØ³Ø¹:**
```yaml
# autoscaling.yaml
services:
  naebak-frontend:
    scaling:
      min_instances: 2
      max_instances: 100
      target_cpu_utilization: 70
      target_concurrency: 1000
      scale_down_delay: 600s  # 10 minutes
      
  naebak-gateway:
    scaling:
      min_instances: 2
      max_instances: 50
      target_cpu_utilization: 75
      target_concurrency: 800
      
  naebak-auth-service:
    scaling:
      min_instances: 1
      max_instances: 20
      target_cpu_utilization: 80
      target_concurrency: 500
      
  naebak-complaints-service:
    scaling:
      min_instances: 1
      max_instances: 30
      target_cpu_utilization: 75
      target_concurrency: 500
```

### **2. Database Auto Scaling**

#### **ØªÙƒÙˆÙŠÙ† Cloud SQL:**
```python
class DatabaseScalingManager:
    """Ø¥Ø¯Ø§Ø±Ø© ØªÙˆØ³Ø¹ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
    
    def __init__(self, project_id, instance_id):
        self.project_id = project_id
        self.instance_id = instance_id
        self.client = sqladmin_v1.SqlInstancesServiceClient()
    
    def monitor_and_scale(self):
        """Ù…Ø±Ø§Ù‚Ø¨Ø© ÙˆØªÙˆØ³ÙŠØ¹ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
        metrics = self.get_database_metrics()
        
        # ÙØ­Øµ Ø§Ø³ØªØ®Ø¯Ø§Ù… CPU
        if metrics['cpu_usage'] > 80:
            self.scale_up_cpu()
        elif metrics['cpu_usage'] < 30:
            self.scale_down_cpu()
        
        # ÙØ­Øµ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø°Ø§ÙƒØ±Ø©
        if metrics['memory_usage'] > 85:
            self.scale_up_memory()
        
        # ÙØ­Øµ Ù…Ø³Ø§Ø­Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ†
        if metrics['storage_usage'] > 90:
            self.increase_storage()
        
        # Ø¥Ø¶Ø§ÙØ© read replicas Ø¹Ù†Ø¯ Ø§Ù„Ø­Ø§Ø¬Ø©
        if metrics['read_load'] > 70:
            self.add_read_replica()
    
    def scale_up_cpu(self):
        """Ø²ÙŠØ§Ø¯Ø© CPU Ù„Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
        current_tier = self.get_current_tier()
        new_tier = self.get_next_tier(current_tier, 'cpu')
        
        if new_tier:
            self.update_instance_tier(new_tier)
            logging.info(f"Scaled up database CPU to {new_tier}")
    
    def add_read_replica(self):
        """Ø¥Ø¶Ø§ÙØ© read replica"""
        replica_config = {
            'name': f"{self.instance_id}-replica-{int(time.time())}",
            'masterInstanceName': self.instance_id,
            'region': 'us-central1',
            'settings': {
                'tier': 'db-custom-2-8192',
                'replicationType': 'READ'
            }
        }
        
        operation = self.client.insert(
            project=self.project_id,
            body=replica_config
        )
        
        logging.info(f"Created read replica: {replica_config['name']}")
```

### **3. Load Balancing**

#### **Ø¥Ø¹Ø¯Ø§Ø¯ Load Balancer:**
```yaml
# load-balancer.yaml
apiVersion: networking.gke.io/v1
kind: ManagedCertificate
metadata:
  name: naebak-ssl-cert
spec:
  domains:
    - naebak.com
    - www.naebak.com
    - api.naebak.com
---
apiVersion: v1
kind: Service
metadata:
  name: naebak-frontend-service
  annotations:
    cloud.google.com/neg: '{"ingress": true}'
    cloud.google.com/backend-config: '{"default": "naebak-backend-config"}'
spec:
  type: ClusterIP
  selector:
    app: naebak-frontend
  ports:
    - port: 80
      targetPort: 8080
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: naebak-ingress
  annotations:
    kubernetes.io/ingress.global-static-ip-name: naebak-ip
    networking.gke.io/managed-certificates: naebak-ssl-cert
    kubernetes.io/ingress.class: gce
spec:
  rules:
    - host: naebak.com
      http:
        paths:
          - path: /*
            pathType: ImplementationSpecific
            backend:
              service:
                name: naebak-frontend-service
                port:
                  number: 80
    - host: api.naebak.com
      http:
        paths:
          - path: /*
            pathType: ImplementationSpecific
            backend:
              service:
                name: naebak-gateway-service
                port:
                  number: 80
```

---

## ğŸ’¾ **Ø§Ù„Ù†Ø³Ø® Ø§Ù„Ø§Ø­ØªÙŠØ§Ø·ÙŠ ÙˆØ§Ù„Ø§Ø³ØªØ¹Ø§Ø¯Ø©**

### **1. Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© Ø§Ù„Ù†Ø³Ø® Ø§Ù„Ø§Ø­ØªÙŠØ§Ø·ÙŠ**

#### **Ø¬Ø¯ÙˆÙ„Ø© Ø§Ù„Ù†Ø³Ø® Ø§Ù„Ø§Ø­ØªÙŠØ§Ø·ÙŠØ©:**
```python
class BackupManager:
    """Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ù†Ø³Ø® Ø§Ù„Ø§Ø­ØªÙŠØ§Ø·ÙŠØ©"""
    
    def __init__(self, project_id):
        self.project_id = project_id
        self.storage_client = storage.Client()
        self.sql_client = sqladmin_v1.SqlBackupRunsServiceClient()
    
    def create_full_backup(self):
        """Ø¥Ù†Ø´Ø§Ø¡ Ù†Ø³Ø®Ø© Ø§Ø­ØªÙŠØ§Ø·ÙŠØ© ÙƒØ§Ù…Ù„Ø©"""
        backup_id = f"full_backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
        tasks = [
            self.backup_database(),
            self.backup_files(),
            self.backup_redis_data(),
            self.backup_configurations()
        ]
        
        # ØªÙ†ÙÙŠØ° Ø§Ù„Ù†Ø³Ø® Ø§Ù„Ø§Ø­ØªÙŠØ§Ø·ÙŠØ© Ø¨Ø§Ù„ØªÙˆØ§Ø²ÙŠ
        with ThreadPoolExecutor(max_workers=4) as executor:
            results = list(executor.map(lambda task: task(), tasks))
        
        # Ø¥Ù†Ø´Ø§Ø¡ manifest file
        manifest = {
            'backup_id': backup_id,
            'timestamp': datetime.now().isoformat(),
            'components': {
                'database': results[0],
                'files': results[1],
                'redis': results[2],
                'configurations': results[3]
            },
            'size_gb': sum(result.get('size_gb', 0) for result in results),
            'status': 'completed' if all(r.get('success') for r in results) else 'failed'
        }
        
        # Ø­ÙØ¸ manifest
        self.save_backup_manifest(backup_id, manifest)
        
        return manifest
    
    def backup_database(self):
        """Ù†Ø³Ø® Ø§Ø­ØªÙŠØ§Ø·ÙŠ Ù„Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
        backup_id = f"db_backup_{int(time.time())}"
        
        request = sqladmin_v1.SqlBackupRunsInsertRequest(
            project=self.project_id,
            instance='naebak-main-db',
            body=sqladmin_v1.BackupRun(
                description=f'Automated backup - {backup_id}',
                type_=sqladmin_v1.BackupRun.Type.ON_DEMAND
            )
        )
        
        operation = self.sql_client.insert(request)
        
        # Ø§Ù†ØªØ¸Ø§Ø± Ø§ÙƒØªÙ…Ø§Ù„ Ø§Ù„Ø¹Ù…Ù„ÙŠØ©
        while not operation.done():
            time.sleep(30)
            operation = self.sql_client.get(
                project=self.project_id,
                operation=operation.name
            )
        
        return {
            'success': operation.status == 'DONE',
            'backup_id': backup_id,
            'size_gb': operation.metadata.get('backupSizeBytes', 0) / (1024**3)
        }
    
    def backup_files(self):
        """Ù†Ø³Ø® Ø§Ø­ØªÙŠØ§Ø·ÙŠ Ù„Ù„Ù…Ù„ÙØ§Øª"""
        source_bucket = 'naebak-media-files'
        backup_bucket = 'naebak-backups'
        backup_folder = f"files_backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
        source_bucket_obj = self.storage_client.bucket(source_bucket)
        backup_bucket_obj = self.storage_client.bucket(backup_bucket)
        
        total_size = 0
        file_count = 0
        
        for blob in source_bucket_obj.list_blobs():
            # Ù†Ø³Ø® Ø§Ù„Ù…Ù„Ù
            source_bucket_obj.copy_blob(
                blob, 
                backup_bucket_obj, 
                f"{backup_folder}/{blob.name}"
            )
            total_size += blob.size
            file_count += 1
        
        return {
            'success': True,
            'backup_folder': backup_folder,
            'file_count': file_count,
            'size_gb': total_size / (1024**3)
        }
    
    def schedule_backups(self):
        """Ø¬Ø¯ÙˆÙ„Ø© Ø§Ù„Ù†Ø³Ø® Ø§Ù„Ø§Ø­ØªÙŠØ§Ø·ÙŠØ©"""
        # Ù†Ø³Ø®Ø© Ø§Ø­ØªÙŠØ§Ø·ÙŠØ© ÙŠÙˆÙ…ÙŠØ© ÙÙŠ Ø§Ù„Ø³Ø§Ø¹Ø© 3 ØµØ¨Ø§Ø­Ø§Ù‹
        schedule.every().day.at("03:00").do(self.create_incremental_backup)
        
        # Ù†Ø³Ø®Ø© Ø§Ø­ØªÙŠØ§Ø·ÙŠØ© ÙƒØ§Ù…Ù„Ø© Ø£Ø³Ø¨ÙˆØ¹ÙŠØ§Ù‹ ÙŠÙˆÙ… Ø§Ù„Ø£Ø­Ø¯
        schedule.every().sunday.at("02:00").do(self.create_full_backup)
        
        # Ù†Ø³Ø®Ø© Ø§Ø­ØªÙŠØ§Ø·ÙŠØ© Ø´Ù‡Ø±ÙŠØ©
        schedule.every().month.do(self.create_monthly_backup)
        
        while True:
            schedule.run_pending()
            time.sleep(3600)  # ÙØ­Øµ ÙƒÙ„ Ø³Ø§Ø¹Ø©
```

### **2. Ø®Ø·Ø© Ø§Ù„Ø§Ø³ØªØ¹Ø§Ø¯Ø©**

#### **Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª Ø§Ù„Ø§Ø³ØªØ¹Ø§Ø¯Ø©:**
```python
class DisasterRecovery:
    """Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ø§Ø³ØªØ¹Ø§Ø¯Ø© Ù…Ù† Ø§Ù„ÙƒÙˆØ§Ø±Ø«"""
    
    def __init__(self, project_id):
        self.project_id = project_id
        self.recovery_time_objective = 15  # 15 minutes RTO
        self.recovery_point_objective = 60  # 1 hour RPO
    
    def execute_disaster_recovery(self, backup_id):
        """ØªÙ†ÙÙŠØ° Ø®Ø·Ø© Ø§Ù„Ø§Ø³ØªØ¹Ø§Ø¯Ø© Ù…Ù† Ø§Ù„ÙƒÙˆØ§Ø±Ø«"""
        recovery_plan = {
            'start_time': datetime.now(),
            'backup_id': backup_id,
            'steps': [
                'validate_backup',
                'prepare_infrastructure',
                'restore_database',
                'restore_files',
                'restore_configurations',
                'validate_services',
                'switch_traffic'
            ]
        }
        
        for step in recovery_plan['steps']:
            try:
                step_start = datetime.now()
                result = getattr(self, step)(backup_id)
                step_duration = (datetime.now() - step_start).total_seconds()
                
                recovery_plan[step] = {
                    'status': 'completed',
                    'duration_seconds': step_duration,
                    'result': result
                }
                
                logging.info(f"Recovery step {step} completed in {step_duration}s")
                
            except Exception as e:
                recovery_plan[step] = {
                    'status': 'failed',
                    'error': str(e)
                }
                logging.error(f"Recovery step {step} failed: {e}")
                break
        
        total_duration = (datetime.now() - recovery_plan['start_time']).total_seconds()
        recovery_plan['total_duration_minutes'] = total_duration / 60
        recovery_plan['rto_met'] = total_duration / 60 <= self.recovery_time_objective
        
        return recovery_plan
    
    def validate_backup(self, backup_id):
        """Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ØµØ­Ø© Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ø§Ø­ØªÙŠØ§Ø·ÙŠØ©"""
        manifest = self.get_backup_manifest(backup_id)
        
        if not manifest:
            raise Exception(f"Backup manifest not found for {backup_id}")
        
        if manifest['status'] != 'completed':
            raise Exception(f"Backup {backup_id} is not complete")
        
        # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…ÙƒÙˆÙ†Ø§Øª
        required_components = ['database', 'files', 'configurations']
        for component in required_components:
            if component not in manifest['components']:
                raise Exception(f"Missing component {component} in backup")
        
        return manifest
    
    def restore_database(self, backup_id):
        """Ø§Ø³ØªØ¹Ø§Ø¯Ø© Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
        # Ø¥Ù†Ø´Ø§Ø¡ instance Ø¬Ø¯ÙŠØ¯ Ù…Ù† Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ø§Ø­ØªÙŠØ§Ø·ÙŠØ©
        restore_instance_id = f"naebak-restore-{int(time.time())}"
        
        request = sqladmin_v1.SqlInstancesCloneRequest(
            project=self.project_id,
            instance='naebak-main-db',
            body=sqladmin_v1.InstancesCloneRequest(
                cloneContext=sqladmin_v1.CloneContext(
                    destinationInstanceName=restore_instance_id,
                    backupRunId=backup_id
                )
            )
        )
        
        operation = self.sql_client.clone(request)
        
        # Ø§Ù†ØªØ¸Ø§Ø± Ø§ÙƒØªÙ…Ø§Ù„ Ø§Ù„Ø§Ø³ØªØ¹Ø§Ø¯Ø©
        while not operation.done():
            time.sleep(30)
            operation = self.sql_client.get(
                project=self.project_id,
                operation=operation.name
            )
        
        return {
            'restored_instance': restore_instance_id,
            'success': operation.status == 'DONE'
        }
```

---

## ğŸ”§ **Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø¨ÙŠØ¦Ø§Øª Ø§Ù„Ù…Ø®ØªÙ„ÙØ©**

### **1. Ø¨ÙŠØ¦Ø© Ø§Ù„ØªØ·ÙˆÙŠØ± (Development)**

```yaml
# development.yaml
environment: development
project_id: naebak-472518-dev

compute:
  min_instances: 1
  max_instances: 5
  cpu: 1
  memory: 2Gi

database:
  tier: db-f1-micro
  storage: 20GB
  backup_enabled: false

monitoring:
  log_level: DEBUG
  metrics_enabled: true
  alerts_enabled: false
```

### **2. Ø¨ÙŠØ¦Ø© Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø± (Staging)**

```yaml
# staging.yaml
environment: staging
project_id: naebak-472518-staging

compute:
  min_instances: 1
  max_instances: 10
  cpu: 1
  memory: 2Gi

database:
  tier: db-custom-2-8192
  storage: 50GB
  backup_enabled: true
  backup_schedule: "0 2 * * *"  # Daily at 2 AM

monitoring:
  log_level: INFO
  metrics_enabled: true
  alerts_enabled: true
```

### **3. Ø¨ÙŠØ¦Ø© Ø§Ù„Ø¥Ù†ØªØ§Ø¬ (Production)**

```yaml
# production.yaml
environment: production
project_id: naebak-472518

compute:
  min_instances: 2
  max_instances: 100
  cpu: 2
  memory: 4Gi

database:
  tier: db-custom-4-16384
  storage: 100GB
  backup_enabled: true
  backup_schedule: "0 3 * * *"  # Daily at 3 AM
  high_availability: true
  read_replicas: 2

monitoring:
  log_level: WARNING
  metrics_enabled: true
  alerts_enabled: true
  uptime_checks: true

security:
  ssl_enabled: true
  waf_enabled: true
  ddos_protection: true
```

---

## ğŸ“‹ **Ø³ÙƒØ±ÙŠØ¨ØªØ§Øª Ø§Ù„Ù†Ø´Ø±**

### **1. Ø³ÙƒØ±ÙŠØ¨Øª Ø§Ù„Ù†Ø´Ø± Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ**

```bash
#!/bin/bash
# deploy.sh

set -e

# Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø¨ÙŠØ¦Ø©
PROJECT_ID="naebak-472518"
REGION="us-central1"
ENVIRONMENT=${1:-production}

echo "ğŸš€ Ø¨Ø¯Ø¡ Ù†Ø´Ø± Ù…Ù†ØµØ© Ù†Ø§Ø¦Ø¨Ùƒ - Ø§Ù„Ø¨ÙŠØ¦Ø©: $ENVIRONMENT"

# Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª
echo "ğŸ“‹ Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª..."
command -v gcloud >/dev/null 2>&1 || { echo "gcloud CLI ØºÙŠØ± Ù…Ø«Ø¨Øª"; exit 1; }
command -v docker >/dev/null 2>&1 || { echo "Docker ØºÙŠØ± Ù…Ø«Ø¨Øª"; exit 1; }

# ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø¯Ø®ÙˆÙ„ ÙˆØªØ¹ÙŠÙŠÙ† Ø§Ù„Ù…Ø´Ø±ÙˆØ¹
echo "ğŸ” Ø¥Ø¹Ø¯Ø§Ø¯ Google Cloud..."
gcloud config set project $PROJECT_ID
gcloud config set run/region $REGION

# Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØµÙˆØ±
echo "ğŸ—ï¸ Ø¨Ù†Ø§Ø¡ Ø§Ù„ØµÙˆØ±..."
./scripts/build-images.sh $ENVIRONMENT

# Ù†Ø´Ø± Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
echo "ğŸ’¾ Ø¥Ø¹Ø¯Ø§Ø¯ Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª..."
./scripts/setup-databases.sh $ENVIRONMENT

# Ù†Ø´Ø± Ø§Ù„Ø®Ø¯Ù…Ø§Øª
echo "â˜ï¸ Ù†Ø´Ø± Ø§Ù„Ø®Ø¯Ù…Ø§Øª..."
./scripts/deploy-services.sh $ENVIRONMENT

# Ø¥Ø¹Ø¯Ø§Ø¯ Load Balancer
echo "âš–ï¸ Ø¥Ø¹Ø¯Ø§Ø¯ Load Balancer..."
./scripts/setup-load-balancer.sh $ENVIRONMENT

# Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù…Ø±Ø§Ù‚Ø¨Ø©
echo "ğŸ“Š Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù…Ø±Ø§Ù‚Ø¨Ø©..."
./scripts/setup-monitoring.sh $ENVIRONMENT

# Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª Ø§Ù„Ù†Ø´Ø±
echo "ğŸ§ª ØªØ´ØºÙŠÙ„ Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª Ø§Ù„Ù†Ø´Ø±..."
./scripts/deployment-tests.sh $ENVIRONMENT

echo "âœ… ØªÙ… Ø§Ù„Ù†Ø´Ø± Ø¨Ù†Ø¬Ø§Ø­!"
echo "ğŸŒ Ø§Ù„Ù…ÙˆÙ‚Ø¹ Ù…ØªØ§Ø­ Ø¹Ù„Ù‰: https://naebak.com"
```

### **2. Ø³ÙƒØ±ÙŠØ¨Øª Ø¨Ù†Ø§Ø¡ Ø§Ù„ØµÙˆØ±**

```bash
#!/bin/bash
# scripts/build-images.sh

ENVIRONMENT=$1
TIMESTAMP=$(date +%Y%m%d_%H%M%S)
REGISTRY="gcr.io/naebak-472518"

services=(
    "frontend"
    "gateway" 
    "auth-service"
    "complaints-service"
    "admin-service"
)

echo "ğŸ—ï¸ Ø¨Ù†Ø§Ø¡ ØµÙˆØ± Docker..."

for service in "${services[@]}"; do
    echo "ğŸ“¦ Ø¨Ù†Ø§Ø¡ ØµÙˆØ±Ø© $service..."
    
    cd $service
    
    # Ø¨Ù†Ø§Ø¡ Ø§Ù„ØµÙˆØ±Ø©
    docker build -t $REGISTRY/$service:$TIMESTAMP .
    docker build -t $REGISTRY/$service:latest .
    
    # Ø±ÙØ¹ Ø§Ù„ØµÙˆØ±Ø©
    docker push $REGISTRY/$service:$TIMESTAMP
    docker push $REGISTRY/$service:latest
    
    echo "âœ… ØªÙ… Ø¨Ù†Ø§Ø¡ ÙˆØ±ÙØ¹ $service"
    
    cd ..
done

echo "ğŸ‰ ØªÙ… Ø¨Ù†Ø§Ø¡ Ø¬Ù…ÙŠØ¹ Ø§Ù„ØµÙˆØ± Ø¨Ù†Ø¬Ø§Ø­!"
```

---

## ğŸ¯ **Ø®Ù„Ø§ØµØ© Ø§Ù„Ø¨Ù†ÙŠØ© Ø§Ù„ØªØ­ØªÙŠØ©**

### **Ø§Ù„Ù…Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…Ø­Ù‚Ù‚Ø©:**

1. **Ø§Ù„ØªÙˆÙØ± Ø§Ù„Ø¹Ø§Ù„ÙŠ** - 99.9% uptime Ù…Ø¹ redundancy ÙƒØ§Ù…Ù„
2. **Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù…ØªÙ…ÙŠØ²** - Ø§Ø³ØªØ¬Ø§Ø¨Ø© Ø£Ù‚Ù„ Ù…Ù† 200ms Ù…Ø¹ CDN Ø¹Ø§Ù„Ù…ÙŠ
3. **Ø§Ù„ØªÙˆØ³Ø¹ Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ** - Ø¯Ø¹Ù… Ù†Ù…Ùˆ Ù…Ù† 100 Ø¥Ù„Ù‰ 100,000 Ù…Ø³ØªØ®Ø¯Ù…
4. **Ø§Ù„Ø£Ù…Ø§Ù† Ø§Ù„Ù…ØªÙ‚Ø¯Ù…** - Ø­Ù…Ø§ÙŠØ© Ø´Ø§Ù…Ù„Ø© Ù…Ø¹ SSL ÙˆWAF
5. **Ø§Ù„Ù…Ø±Ø§Ù‚Ø¨Ø© Ø§Ù„Ø°ÙƒÙŠØ©** - ØªÙ†Ø¨ÙŠÙ‡Ø§Øª ÙÙˆØ±ÙŠØ© ÙˆÙ…Ù‚Ø§ÙŠÙŠØ³ Ù…Ø®ØµØµØ©
6. **Ø§Ù„Ù†Ø³Ø® Ø§Ù„Ø§Ø­ØªÙŠØ§Ø·ÙŠ** - Ø§Ø³ØªØ¹Ø§Ø¯Ø© Ø®Ù„Ø§Ù„ 15 Ø¯Ù‚ÙŠÙ‚Ø©
7. **Ø§Ù„Ù†Ø´Ø± Ø§Ù„Ù…ØªÙ‚Ø¯Ù…** - Blue-Green deployment Ù…Ø¹ rollback ØªÙ„Ù‚Ø§Ø¦ÙŠ

### **Ø§Ù„ØªÙƒÙ„ÙØ© Ø§Ù„Ù…ØªÙˆÙ‚Ø¹Ø©:**

| Ø§Ù„Ù…ÙƒÙˆÙ† | Ø§Ù„ØªÙƒÙ„ÙØ© Ø§Ù„Ø´Ù‡Ø±ÙŠØ© (USD) |
|--------|---------------------|
| Cloud Run (Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø®Ø¯Ù…Ø§Øª) | $200-500 |
| Cloud SQL (PostgreSQL) | $150-300 |
| Cloud Storage | $50-100 |
| Redis (Memorystore) | $100-200 |
| Load Balancer | $20-50 |
| Monitoring & Logging | $50-100 |
| **Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹** | **$570-1,250** |

### **Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©:**

**Ø¨Ù†ÙŠØ© ØªØ­ØªÙŠØ© Ø¹Ø§Ù„Ù…ÙŠØ© Ø§Ù„Ù…Ø³ØªÙˆÙ‰** ØªØ¯Ø¹Ù… Ù†Ù…Ùˆ Ù…Ù†ØµØ© Ù†Ø§Ø¦Ø¨Ùƒ Ù…Ù† Ù…Ø¦Ø§Øª Ø¥Ù„Ù‰ Ù…Ù„Ø§ÙŠÙŠÙ† Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ†ØŒ Ù…Ø¹ Ø¶Ù…Ø§Ù† Ø§Ù„Ø§Ø³ØªÙ‚Ø±Ø§Ø± ÙˆØ§Ù„Ø£Ù…Ø§Ù† ÙˆØ§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ø¹Ø§Ù„ÙŠ ÙÙŠ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø£ÙˆÙ‚Ø§Øª! ğŸš€
